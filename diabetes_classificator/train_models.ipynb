{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    make_scorer\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Importações para reamostragem\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.metrics import geometric_mean_score # Importa G-Mean do imblearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leitura dos Dados\n",
    "# Carrega o dataset\n",
    "df = pd.read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "\n",
    "# Separa features (X) e target (y)\n",
    "X = df.drop(columns=['Diabetes_binary'])\n",
    "y = df['Diabetes_binary']\n",
    "\n",
    "# Divide os dados em conjuntos de treino e teste, mantendo a proporção das classes\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização min-max dos dados não binários e dos que possuem ordem \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_normalize = [\"BMI\",\"GenHlth\",\"MentHlth\",\"PhysHlth\",\"Age\",\"Education\",\"Income\"]\n",
    "columns_to_pass_through = [\"HighBP\",\"HighChol\",\"CholCheck\",\"Stroke\",\"HeartDiseaseorAttack\",\"PhysActivity\",\"Fruits\",\"Veggies\",\"HvyAlcoholConsump\",\"AnyHealthcare\",\"NoDocbcCost\", \"DiffWalk\",\"Sex\"]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('minmax_scaler', MinMaxScaler(), columns_to_normalize),\n",
    "        ('passthrough_features', 'passthrough', columns_to_pass_through)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métrica para escolha: f1-score na classe minoritária e Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Métrica para escolha: f1-score na classe minoritária\n",
    "# Define o scorer para GridSearchCV/RandomizedSearchCV, focando no f1-score da classe minoritária (1.0)\n",
    "scorer = make_scorer(f1_score, pos_label=1.0)\n",
    "# Define a estratégia de validação cruzada estratificada para manter a proporção das classes\n",
    "cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas para Avaliação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para avaliar e imprimir todas as métricas solicitadas\n",
    "def evaluate_model(y_true, y_pred, y_prob=None, model_name=\"Modelo\"):\n",
    "    \"\"\"\n",
    "    Calcula e exibe métricas de classificação para um modelo.\n",
    "\n",
    "    Args:\n",
    "        y_true (array-like): Rótulos verdadeiros.\n",
    "        y_pred (array-like): Previsões do modelo.\n",
    "        y_prob (array-like, optional): Probabilidades previstas para a classe positiva.\n",
    "                                       Necessário para ROC AUC e Curva ROC. Defaults to None.\n",
    "        model_name (str, optional): Nome do modelo para exibição nos resultados. Defaults to \"Modelo\".\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Métricas para {model_name} ---\")\n",
    "\n",
    "    # Matriz de Confusão\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(\"\\nMatriz de Confusão:\")\n",
    "    print(cm)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Previsto 0', 'Previsto 1'], yticklabels=['Real 0', 'Real 1'])\n",
    "    plt.title(f'Matriz de Confusão para {model_name}')\n",
    "    plt.xlabel('Rótulo Previsto')\n",
    "    plt.ylabel('Rótulo Real')\n",
    "    plt.show()\n",
    "\n",
    "    # Métricas por classe\n",
    "    # precision_score, recall_score, f1_score com average=None retornam os valores por classe\n",
    "    precision_class_0, precision_class_1 = precision_score(y_true, y_pred, average=None)\n",
    "    recall_class_0, recall_class_1 = recall_score(y_true, y_pred, average=None)\n",
    "    f1_class_0, f1_class_1 = f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "    print(f\"\\nMétricas para a Classe 0 (Majoritária):\")\n",
    "    print(f\"  Precisão: {precision_class_0:.4f}\")\n",
    "    print(f\"  Recall: {recall_class_0:.4f}\")\n",
    "    print(f\"  F1-Score: {f1_class_0:.4f}\")\n",
    "\n",
    "    print(f\"\\nMétricas para a Classe 1 (Minoritária):\")\n",
    "    print(f\"  Precisão: {precision_class_1:.4f}\")\n",
    "    print(f\"  Recall: {recall_class_1:.4f}\")\n",
    "    print(f\"  F1-Score: {f1_class_1:.4f}\")\n",
    "\n",
    "    # Métricas gerais\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # f1_score com average='macro' calcula a média não ponderada do f1-score por classe\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    # G-Mean para classes desbalanceadas\n",
    "    g_mean = geometric_mean_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    print(f\"\\nMétricas Gerais:\")\n",
    "    print(f\"  Acurácia: {accuracy:.4f}\")\n",
    "    print(f\"  F1-Score (Macro): {f1_macro:.4f}\")\n",
    "    print(f\"  G-Mean: {g_mean:.4f}\")\n",
    "\n",
    "    # ROC AUC e Curva ROC\n",
    "    # Verifica se y_prob foi fornecido e se é uma classificação binária\n",
    "    if y_prob is not None and len(np.unique(y_true)) == 2:\n",
    "        roc_auc = roc_auc_score(y_true, y_prob)\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "        print(f\"  ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (área = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--') # Linha de classificação aleatória\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('Taxa de Falsos Positivos (FPR)')\n",
    "        plt.ylabel('Taxa de Verdadeiros Positivos (TPR)')\n",
    "        plt.title(f'Curva Característica de Operação do Receptor (ROC) para {model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"  ROC AUC e Curva ROC não aplicáveis (não é classificação binária ou y_prob não fornecido).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento para árvore de decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de Hiperparâmetro com Quatro Tipos de Métodos de Reamostragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline básico (sem reamostragem)\n",
    "pipeline_no_sampling = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com SMOTE\n",
    "pipeline_smote = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', SMOTE(random_state=42)),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com ADASYN\n",
    "pipeline_adasyn = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', ADASYN(random_state=42)),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com Undersampling seguido por SMOTE\n",
    "pipeline_under_smote = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('under_sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('over_sampler', SMOTE(random_state=42)),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com Undersampling seguido por ADASYN\n",
    "pipeline_under_adasyn = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('under_sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('over_sampler', ADASYN(random_state=42)),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Lista de pipelines para experimentação com Árvore de Decisão\n",
    "pipelines_dt = {\n",
    "    'No Sampling': pipeline_no_sampling,\n",
    "    'SMOTE': pipeline_smote,\n",
    "    'ADASYN': pipeline_adasyn,\n",
    "    'Under+SMOTE': pipeline_under_smote,\n",
    "    'Under+ADASYN': pipeline_under_adasyn\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparâmetros: Número de Vizinhos (para os métodos SMOTE e ADASYN), Proporção da classe majoritária (undersampling), Altura de Árvore e Índice de Impureza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: sem otimização de hiperparâmetros (Árvore de Decisão)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Desempenho Inicial dos Pipelines (sem otimização de hiperparâmetros) ---\n",
      "\n",
      "No Sampling:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.88      0.88     54583\n",
      "         1.0       0.30      0.33      0.31      8837\n",
      "\n",
      "    accuracy                           0.80     63420\n",
      "   macro avg       0.59      0.60      0.60     63420\n",
      "weighted avg       0.81      0.80      0.80     63420\n",
      "\n",
      "F1-Score (macro): 0.5966\n",
      "ROC AUC: 0.6005\n",
      "\n",
      "SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.87      0.88     54583\n",
      "         1.0       0.29      0.32      0.30      8837\n",
      "\n",
      "    accuracy                           0.80     63420\n",
      "   macro avg       0.59      0.60      0.59     63420\n",
      "weighted avg       0.80      0.80      0.80     63420\n",
      "\n",
      "F1-Score (macro): 0.5911\n",
      "ROC AUC: 0.5954\n",
      "\n",
      "ADASYN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.87      0.88     54583\n",
      "         1.0       0.28      0.31      0.30      8837\n",
      "\n",
      "    accuracy                           0.79     63420\n",
      "   macro avg       0.59      0.59      0.59     63420\n",
      "weighted avg       0.80      0.79      0.80     63420\n",
      "\n",
      "F1-Score (macro): 0.5884\n",
      "ROC AUC: 0.5922\n",
      "\n",
      "Under+SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.66      0.77     54583\n",
      "         1.0       0.24      0.64      0.34      8837\n",
      "\n",
      "    accuracy                           0.66     63420\n",
      "   macro avg       0.58      0.65      0.56     63420\n",
      "weighted avg       0.82      0.66      0.71     63420\n",
      "\n",
      "F1-Score (macro): 0.5571\n",
      "ROC AUC: 0.6516\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Desempenho Inicial dos Pipelines (Árvore de Decisão - sem otimização de hiperparâmetros) ---\")\n",
    "for name, pipeline in pipelines_dt.items():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1] # Obtém probabilidades para ROC AUC\n",
    "    evaluate_model(y_test, y_pred, y_prob, f\"Árvore de Decisão - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (Sem reamostragem - Árvore de Decisão)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_dt = {\n",
    "    'classifier__max_depth': [3, 5, 7, 9, 11, 13, 14, None], # Altura da árvore\n",
    "    'classifier__criterion': ['gini', 'entropy'], # Índice de pureza\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Otimizando Pipeline Sem Reamostragem...\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "Melhores parâmetros Sem Reamostragem: {'classifier__criterion': 'entropy', 'classifier__max_depth': None}\n",
      "Relatório de Classificação Sem Reamostragem:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.88      0.88     54583\n",
      "         1.0       0.30      0.32      0.31      8837\n",
      "\n",
      "    accuracy                           0.80     63420\n",
      "   macro avg       0.60      0.60      0.60     63420\n",
      "weighted avg       0.81      0.80      0.81     63420\n",
      "\n",
      "F1-Score (macro): 0.5989\n",
      "ROC AUC: 0.6015\n"
     ]
    }
   ],
   "source": [
    "# --- Otimização para Sem Reamostragem (Árvore de Decisão) ---\n",
    "print(\"\\nOtimizando Pipeline Sem Reamostragem (Árvore de Decisão)...\")\n",
    "grid_dt_no_sampling = RandomizedSearchCV(pipeline_no_sampling, param_grid_dt, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, n_iter=10, random_state=42)\n",
    "\n",
    "grid_dt_no_sampling.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros Sem Reamostragem (Árvore de Decisão): {grid_dt_no_sampling.best_params_}\")\n",
    "y_pred_tuned = grid_dt_no_sampling.best_estimator_.predict(X_test)\n",
    "y_prob_tuned = grid_dt_no_sampling.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_tuned, y_prob_tuned, \"Árvore de Decisão - Sem Reamostragem (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (SMOTE - Árvore de Decisão)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_smote_dt = {\n",
    "    'sampler__k_neighbors': [3, 5, 7, 10],  # k_neighbors para SMOTE\n",
    "    'classifier__max_depth': [3, 5, 7, 10, None], # Altura da árvore\n",
    "    'classifier__criterion': ['gini', 'entropy'], # Índice de pureza\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Otimizando Pipeline com SMOTE...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid_smote_dt = {\n",
    "    'sampler__k_neighbors': [3, 5, 7, 10],  # k_neighbors para SMOTE\n",
    "    'classifier__max_depth': [3, 5, 7, 10, None], # Altura da árvore\n",
    "    'classifier__criterion': ['gini', 'entropy'], # Índice de pureza\n",
    "}\n",
    "\n",
    "# --- Otimização para SMOTE (Árvore de Decisão) ---\n",
    "print(\"\\nOtimizando Pipeline com SMOTE (Árvore de Decisão)...\")\n",
    "grid_dt_smote = RandomizedSearchCV(pipeline_smote, param_grid_smote_dt, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_dt_smote.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para SMOTE (Árvore de Decisão): {grid_dt_smote.best_params_}\")\n",
    "y_pred_smote_tuned = grid_dt_smote.best_estimator_.predict(X_test)\n",
    "y_prob_smote_tuned = grid_dt_smote.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_smote_tuned, y_prob_smote_tuned, \"Árvore de Decisão - SMOTE (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (ADASYN - Árvore de Decisão)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_adasyn_dt = {\n",
    "    'sampler__n_neighbors': [3, 5, 7, 10], # n_neighbors para ADASYN\n",
    "    'classifier__max_depth': [3, 5, 7, 10, None],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para ADASYN (Árvore de Decisão) ---\n",
    "print(\"\\nOtimizando Pipeline com ADASYN (Árvore de Decisão)...\")\n",
    "grid_dt_adasyn = RandomizedSearchCV(pipeline_adasyn, param_grid_adasyn_dt, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_dt_adasyn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para ADASYN (Árvore de Decisão): {grid_dt_adasyn.best_params_}\")\n",
    "y_pred_adasyn_tuned = grid_dt_adasyn.best_estimator_.predict(X_test)\n",
    "y_prob_adasyn_tuned = grid_dt_adasyn.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_adasyn_tuned, y_prob_adasyn_tuned, \"Árvore de Decisão - ADASYN (Otimizado)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (SMOTE + Undersampling - Árvore de Decisão)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_under_smote_dt = {\n",
    "    'under_sampler__sampling_strategy': [0.5, 0.7, 0.9], # Proporção da classe majoritária após undersampling\n",
    "    'over_sampler__k_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__max_depth': [3, 5, 7, 10, None],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para SMOTE + Undersampling (Árvore de Decisão) ---\n",
    "print(\"\\nOtimizando Pipeline com SMOTE + Undersampling (Árvore de Decisão)...\")\n",
    "grid_dt_under_smote = RandomizedSearchCV(pipeline_under_smote, param_grid_under_smote_dt, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_dt_under_smote.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para SMOTE + Undersampling (Árvore de Decisão): {grid_dt_under_smote.best_params_}\")\n",
    "y_pred_under_smote_tuned = grid_dt_under_smote.best_estimator_.predict(X_test)\n",
    "y_prob_under_smote_tuned = grid_dt_under_smote.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_under_smote_tuned, y_prob_under_smote_tuned, \"Árvore de Decisão - Under+SMOTE (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (ADASYN + Undersampling - Árvore de Decisão)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_under_adasyn_dt = {\n",
    "    'under_sampler__sampling_strategy': [0.5, 0.7, 0.9], # Proporção da classe majoritária após undersampling\n",
    "    'over_sampler__n_neighbors': [3, 5, 7, 10],\n",
    "    'classifier__max_depth': [3, 5, 7, 10, None],\n",
    "    'classifier__criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para ADASYN + Undersampling (Árvore de Decisão) ---\n",
    "print(\"\\nOtimizando Pipeline com ADASYN + Undersampling (Árvore de Decisão)...\")\n",
    "grid_dt_under_adasyn = RandomizedSearchCV(pipeline_under_adasyn, param_grid_under_adasyn_dt, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_dt_under_adasyn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para ADASYN + Undersampling (Árvore de Decisão): {grid_dt_under_adasyn.best_params_}\")\n",
    "y_pred_under_adasyn_tuned = grid_dt_under_adasyn.best_estimator_.predict(X_test)\n",
    "y_prob_under_adasyn_tuned = grid_dt_under_adasyn.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_under_adasyn_tuned, y_prob_under_adasyn_tuned, \"Árvore de Decisão - Under+ADASYN (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento para AdaBoot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de Hiperparâmetro com Quatro Tipos de Métodos de Reamostragem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline básico (sem reamostragem) para AdaBoost\n",
    "pipeline_no_sampling_ada = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('adaboost', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com SMOTE para AdaBoost\n",
    "pipeline_smote_ada = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', SMOTE(random_state=42)),\n",
    "    ('adaboost', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com ADASYN para AdaBoost\n",
    "pipeline_adasyn_ada = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', ADASYN(random_state=42)),\n",
    "    ('adaboost', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com Undersampling seguido por SMOTE para AdaBoost\n",
    "pipeline_under_smote_ada = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('under_sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('over_sampler', SMOTE(random_state=42)),\n",
    "    ('adaboost', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com Undersampling seguido por ADASYN para AdaBoost\n",
    "pipeline_under_adasyn_ada = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('under_sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('over_sampler', ADASYN(random_state=42)),\n",
    "    ('adaboost', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42))\n",
    "])\n",
    "\n",
    "# Lista de pipelines para experimentação (AdaBoost)\n",
    "pipelines_ada = {\n",
    "    'No Sampling': pipeline_no_sampling_ada,\n",
    "    'SMOTE': pipeline_smote_ada,\n",
    "    'ADASYN': pipeline_adasyn_ada,\n",
    "    'Under+SMOTE': pipeline_under_smote_ada,\n",
    "    'Under+ADASYN': pipeline_under_adasyn_ada\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: sem otimização de hiperparâmetros (AdaBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Desempenho Inicial dos Pipelines (sem otimização de hiperparâmetros) ---\n",
      "\n",
      "No Sampling:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.97      0.92     54583\n",
      "         1.0       0.52      0.18      0.27      8837\n",
      "\n",
      "    accuracy                           0.86     63420\n",
      "   macro avg       0.70      0.58      0.60     63420\n",
      "weighted avg       0.83      0.86      0.83     63420\n",
      "\n",
      "F1-Score (macro): 0.5975\n",
      "ROC AUC: 0.5778\n",
      "\n",
      "SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.75      0.84     54583\n",
      "         1.0       0.32      0.72      0.44      8837\n",
      "\n",
      "    accuracy                           0.75     63420\n",
      "   macro avg       0.63      0.73      0.64     63420\n",
      "weighted avg       0.86      0.75      0.78     63420\n",
      "\n",
      "F1-Score (macro): 0.6400\n",
      "ROC AUC: 0.7348\n",
      "\n",
      "ADASYN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.75      0.84     54583\n",
      "         1.0       0.32      0.72      0.44      8837\n",
      "\n",
      "    accuracy                           0.75     63420\n",
      "   macro avg       0.63      0.73      0.64     63420\n",
      "weighted avg       0.86      0.75      0.78     63420\n",
      "\n",
      "F1-Score (macro): 0.6376\n",
      "ROC AUC: 0.7339\n",
      "\n",
      "Under+SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.73      0.83     54583\n",
      "         1.0       0.31      0.75      0.44      8837\n",
      "\n",
      "    accuracy                           0.74     63420\n",
      "   macro avg       0.63      0.74      0.64     63420\n",
      "weighted avg       0.86      0.74      0.77     63420\n",
      "\n",
      "F1-Score (macro): 0.6351\n",
      "ROC AUC: 0.7428\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Desempenho Inicial dos Pipelines (AdaBoost - sem otimização de hiperparâmetros) ---\")\n",
    "for name, pipeline in pipelines_ada.items():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "    evaluate_model(y_test, y_pred, y_prob, f\"AdaBoost - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (Sem reamostragem - AdaBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ada = {\n",
    "    'adaboost__n_estimators': [50, 100, 200],  # Número de estimadores fracos (árvores)\n",
    "    'adaboost__learning_rate': [0.01, 0.1, 1.0],  # Contribuição de cada estimador\n",
    "    'adaboost__estimator__max_depth': [1, 2, 3],  # Profundidade máxima da árvore no estimador base\n",
    "    'adaboost__estimator__criterion': ['gini', 'entropy'] # Critério de pureza para o estimador base\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Otimizando Pipeline Sem Reamostragem...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Melhores parâmetros Sem Reamostragem: {'adaboost__estimator__criterion': 'gini', 'adaboost__estimator__max_depth': 3, 'adaboost__learning_rate': 0.1, 'adaboost__n_estimators': 200}\n",
      "Relatório de Classificação Sem Reamostragem:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.97      0.92     54583\n",
      "         1.0       0.52      0.20      0.29      8837\n",
      "\n",
      "    accuracy                           0.86     63420\n",
      "   macro avg       0.70      0.58      0.61     63420\n",
      "weighted avg       0.83      0.86      0.84     63420\n",
      "\n",
      "F1-Score (macro): 0.6063\n",
      "ROC AUC: 0.5850\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOtimizando Pipeline Sem Reamostragem (AdaBoost)...\")\n",
    "grid_ada_no_sampling = RandomizedSearchCV(pipeline_no_sampling_ada, param_grid_ada, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, n_iter=10, random_state=42)\n",
    "\n",
    "grid_ada_no_sampling.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros Sem Reamostragem (AdaBoost): {grid_ada_no_sampling.best_params_}\")\n",
    "y_pred_tuned = grid_ada_no_sampling.best_estimator_.predict(X_test)\n",
    "y_prob_tuned = grid_ada_no_sampling.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_tuned, y_prob_tuned, \"AdaBoost - Sem Reamostragem (Otimizado)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (SMOTE - AdaBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_smote_ada = {\n",
    "    'sampler__k_neighbors': [3, 5, 7, 10], # k_neighbors para SMOTE\n",
    "    'adaboost__n_estimators': [50, 100, 200],\n",
    "    'adaboost__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'adaboost__estimator__max_depth': [1, 2, 3],\n",
    "    'adaboost__estimator__criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Otimizando Pipeline Sem Reamostragem...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOtimizando Pipeline Sem Reamostragem...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline_smote_ada, param_grid_ada, cv\u001b[38;5;241m=\u001b[39mcv_strategy,\n\u001b[1;32m      8\u001b[0m                           scoring\u001b[38;5;241m=\u001b[39mscorer, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# n_jobs=-1 usa todos os núcleos\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMelhores parâmetros Sem Reamostragem: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m y_pred_tuned \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m   1020\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    967\u001b[0m         )\n\u001b[1;32m    968\u001b[0m     )\n\u001b[0;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    993\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- Otimização para SMOTE (AdaBoost) ---\n",
    "print(\"\\nOtimizando Pipeline com SMOTE (AdaBoost)...\")\n",
    "grid_ada_smote = RandomizedSearchCV(pipeline_smote_ada, param_grid_smote_ada, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_ada_smote.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para SMOTE (AdaBoost): {grid_ada_smote.best_params_}\")\n",
    "y_pred_smote_tuned = grid_ada_smote.best_estimator_.predict(X_test)\n",
    "y_prob_smote_tuned = grid_ada_smote.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_smote_tuned, y_prob_smote_tuned, \"AdaBoost - SMOTE (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (ADASYN - AdaBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_adasyn_ada = {\n",
    "    'sampler__n_neighbors': [3, 5, 7, 10], # n_neighbors para ADASYN\n",
    "    'adaboost__n_estimators': [50, 100, 200],\n",
    "    'adaboost__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'adaboost__estimator__max_depth': [1, 2, 3],\n",
    "    'adaboost__estimator__criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para ADASYN (AdaBoost) ---\n",
    "print(\"\\nOtimizando Pipeline com ADASYN (AdaBoost)...\")\n",
    "grid_ada_adasyn = RandomizedSearchCV(pipeline_adasyn_ada, param_grid_adasyn_ada, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_ada_adasyn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para ADASYN (AdaBoost): {grid_ada_adasyn.best_params_}\")\n",
    "y_pred_adasyn_tuned = grid_ada_adasyn.best_estimator_.predict(X_test)\n",
    "y_prob_adasyn_tuned = grid_ada_adasyn.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_adasyn_tuned, y_prob_adasyn_tuned, \"AdaBoost - ADASYN (Otimizado)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (SMOTE + Undersampling - AdaBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_under_smote_ada = {\n",
    "    'under_sampler__sampling_strategy': [0.5, 0.7, 0.9],\n",
    "    'over_sampler__k_neighbors': [3, 5, 7, 10],\n",
    "    'adaboost__n_estimators': [50, 100, 200],\n",
    "    'adaboost__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'adaboost__estimator__max_depth': [1, 2, 3],\n",
    "    'adaboost__estimator__criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para SMOTE + Undersampling (AdaBoost) ---\n",
    "print(\"\\nOtimizando Pipeline com SMOTE + Undersampling (AdaBoost)...\")\n",
    "grid_ada_under_smote = RandomizedSearchCV(pipeline_under_smote_ada, param_grid_under_smote_ada, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_ada_under_smote.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para SMOTE + Undersampling (AdaBoost): {grid_ada_under_smote.best_params_}\")\n",
    "y_pred_under_smote_tuned = grid_ada_under_smote.best_estimator_.predict(X_test)\n",
    "y_prob_under_smote_tuned = grid_ada_under_smote.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_under_smote_tuned, y_prob_under_smote_tuned, \"AdaBoost - Under+SMOTE (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (ADASYN + Undersampling - AdaBoost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_under_adasyn_ada = {\n",
    "    'under_sampler__sampling_strategy': [0.5, 0.7, 0.9],\n",
    "    'over_sampler__n_neighbors': [3, 5, 7, 10],\n",
    "    'adaboost__n_estimators': [50, 100, 200],\n",
    "    'adaboost__learning_rate': [0.01, 0.1, 1.0],\n",
    "    'adaboost__estimator__max_depth': [1, 2, 3],\n",
    "    'adaboost__estimator__criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para ADASYN + Undersampling (AdaBoost) ---\n",
    "print(\"\\nOtimizando Pipeline com ADASYN + Undersampling (AdaBoost)...\")\n",
    "grid_ada_under_adasyn = RandomizedSearchCV(pipeline_under_adasyn_ada, param_grid_under_adasyn_ada, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_ada_under_adasyn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para ADASYN + Undersampling (AdaBoost): {grid_ada_under_adasyn.best_params_}\")\n",
    "y_pred_under_adasyn_tuned = grid_ada_under_adasyn.best_estimator_.predict(X_test)\n",
    "y_prob_under_adasyn_tuned = grid_ada_under_adasyn.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_under_adasyn_tuned, y_prob_under_adasyn_tuned, \"AdaBoost - Under+ADASYN (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento para GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de Hiperparâmetro com Quatro Tipos de Métodos de Reamostragem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline básico (sem reamostragem) para GradientBoosting\n",
    "pipeline_no_sampling_gb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('gradientboost', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com SMOTE para GradientBoosting\n",
    "pipeline_smote_gb = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', SMOTE(random_state=42)),\n",
    "    ('gradientboost', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com ADASYN para GradientBoosting\n",
    "pipeline_adasyn_gb = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('sampler', ADASYN(random_state=42)),\n",
    "    ('gradientboost', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com Undersampling seguido por SMOTE para GradientBoosting\n",
    "pipeline_under_smote_gb = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('under_sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('over_sampler', SMOTE(random_state=42)),\n",
    "    ('gradientboost', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Pipeline com Undersampling seguido por ADASYN para GradientBoosting\n",
    "pipeline_under_adasyn_gb = ImbPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('under_sampler', RandomUnderSampler(random_state=42)),\n",
    "    ('over_sampler', ADASYN(random_state=42)),\n",
    "    ('gradientboost', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Lista de pipelines para experimentação com GradientBoosting\n",
    "pipelines_gb = {\n",
    "    'No Sampling': pipeline_no_sampling_gb,\n",
    "    'SMOTE': pipeline_smote_gb,\n",
    "    'ADASYN': pipeline_adasyn_gb,\n",
    "    'Under+SMOTE': pipeline_under_smote_gb,\n",
    "    'Under+ADASYN': pipeline_under_adasyn_gb\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: sem otimização de hiperparâmetros (Gradient Boosting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Desempenho Inicial dos Pipelines (sem otimização de hiperparâmetros) ---\n",
      "\n",
      "No Sampling:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.98      0.93     54583\n",
      "         1.0       0.55      0.16      0.25      8837\n",
      "\n",
      "    accuracy                           0.86     63420\n",
      "   macro avg       0.71      0.57      0.59     63420\n",
      "weighted avg       0.83      0.86      0.83     63420\n",
      "\n",
      "F1-Score (macro): 0.5879\n",
      "ROC AUC: 0.5703\n",
      "\n",
      "SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.88      0.90     54583\n",
      "         1.0       0.41      0.49      0.45      8837\n",
      "\n",
      "    accuracy                           0.83     63420\n",
      "   macro avg       0.66      0.69      0.67     63420\n",
      "weighted avg       0.84      0.83      0.84     63420\n",
      "\n",
      "F1-Score (macro): 0.6728\n",
      "ROC AUC: 0.6884\n",
      "\n",
      "ADASYN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.88      0.90     54583\n",
      "         1.0       0.40      0.49      0.44      8837\n",
      "\n",
      "    accuracy                           0.83     63420\n",
      "   macro avg       0.66      0.69      0.67     63420\n",
      "weighted avg       0.84      0.83      0.84     63420\n",
      "\n",
      "F1-Score (macro): 0.6706\n",
      "ROC AUC: 0.6858\n",
      "\n",
      "Under+SMOTE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.71      0.81     54583\n",
      "         1.0       0.31      0.79      0.44      8837\n",
      "\n",
      "    accuracy                           0.72     63420\n",
      "   macro avg       0.63      0.75      0.63     63420\n",
      "weighted avg       0.86      0.72      0.76     63420\n",
      "\n",
      "F1-Score (macro): 0.6288\n",
      "ROC AUC: 0.7521\n",
      "\n",
      "Under+ADASYN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.71      0.81     54583\n",
      "         1.0       0.31      0.79      0.44      8837\n",
      "\n",
      "    accuracy                           0.72     63420\n",
      "   macro avg       0.63      0.75      0.63     63420\n",
      "weighted avg       0.86      0.72      0.76     63420\n",
      "\n",
      "F1-Score (macro): 0.6288\n",
      "ROC AUC: 0.7521\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Desempenho Inicial dos Pipelines (Gradient Boosting - sem otimização de hiperparâmetros) ---\")\n",
    "for name, pipeline in pipelines_gb.items():\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "    evaluate_model(y_test, y_pred, y_prob, f\"Gradient Boosting - {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (Sem reamostragem - Gradient Boosting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_gb = {\n",
    "    'gradientboost__n_estimators': [50, 100, 200],\n",
    "    'gradientboost__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gradientboost__max_depth': [3, 5, 7],\n",
    "    'gradientboost__min_samples_split': [2, 5],\n",
    "    'gradientboost__min_samples_leaf': [1, 3],\n",
    "    'gradientboost__subsample': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para Sem Reamostragem (Gradient Boosting) ---\n",
    "print(\"\\nOtimizando Pipeline Sem Reamostragem (Gradient Boosting)...\")\n",
    "grid_gb_no_sampling = RandomizedSearchCV(pipeline_no_sampling_gb, param_grid_gb, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, n_iter=10, random_state=42)\n",
    "\n",
    "grid_gb_no_sampling.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros Sem Reamostragem (Gradient Boosting): {grid_gb_no_sampling.best_params_}\")\n",
    "y_pred_tuned = grid_gb_no_sampling.best_estimator_.predict(X_test)\n",
    "y_prob_tuned = grid_gb_no_sampling.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_tuned, y_prob_tuned, \"Gradient Boosting - Sem Reamostragem (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (SMOTE - Gradient Boosting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_smote_gb = {\n",
    "    'sampler__k_neighbors': [3, 5, 7, 10],\n",
    "    'gradientboost__n_estimators': [50, 100, 200],\n",
    "    'gradientboost__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gradientboost__max_depth': [3, 5, 7],\n",
    "    'gradientboost__min_samples_split': [2, 5],\n",
    "    'gradientboost__min_samples_leaf': [1, 3],\n",
    "    'gradientboost__subsample': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para SMOTE (Gradient Boosting) ---\n",
    "print(\"\\nOtimizando Pipeline com SMOTE (Gradient Boosting)...\")\n",
    "grid_gb_smote = RandomizedSearchCV(pipeline_smote_gb, param_grid_smote_gb, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_gb_smote.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para SMOTE (Gradient Boosting): {grid_gb_smote.best_params_}\")\n",
    "y_pred_smote_tuned = grid_gb_smote.best_estimator_.predict(X_test)\n",
    "y_prob_smote_tuned = grid_gb_smote.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_smote_tuned, y_prob_smote_tuned, \"Gradient Boosting - SMOTE (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (ADASYN - Gradient Boosting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_adasyn_gb = {\n",
    "    'sampler__n_neighbors': [3, 5, 7, 10],\n",
    "    'gradientboost__n_estimators': [50, 100, 200],\n",
    "    'gradientboost__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gradientboost__max_depth': [3, 5, 7],\n",
    "    'gradientboost__min_samples_split': [2, 5],\n",
    "    'gradientboost__min_samples_leaf': [1, 3],\n",
    "    'gradientboost__subsample': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para ADASYN (Gradient Boosting) ---\n",
    "print(\"\\nOtimizando Pipeline com ADASYN (Gradient Boosting)...\")\n",
    "grid_gb_adasyn = RandomizedSearchCV(pipeline_adasyn_gb, param_grid_adasyn_gb, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_gb_adasyn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para ADASYN (Gradient Boosting): {grid_gb_adasyn.best_params_}\")\n",
    "y_pred_adasyn_tuned = grid_gb_adasyn.best_estimator_.predict(X_test)\n",
    "y_prob_adasyn_tuned = grid_gb_adasyn.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_adasyn_tuned, y_prob_adasyn_tuned, \"Gradient Boosting - ADASYN (Otimizado)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (SMOTE + Undersampling - Gradient Boosting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_under_smote_gb = {\n",
    "    'under_sampler__sampling_strategy': [0.5, 0.7, 0.9],\n",
    "    'over_sampler__k_neighbors': [3, 5, 7, 10],\n",
    "    'gradientboost__n_estimators': [50, 100, 200],\n",
    "    'gradientboost__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gradientboost__max_depth': [3, 5, 7],\n",
    "    'gradientboost__min_samples_split': [2, 5],\n",
    "    'gradientboost__min_samples_leaf': [1, 3],\n",
    "    'gradientboost__subsample': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para SMOTE + Undersampling (Gradient Boosting) ---\n",
    "print(\"\\nOtimizando Pipeline com SMOTE + Undersampling (Gradient Boosting)...\")\n",
    "grid_gb_under_smote = RandomizedSearchCV(pipeline_under_smote_gb, param_grid_under_smote_gb, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_gb_under_smote.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para SMOTE + Undersampling (Gradient Boosting): {grid_gb_under_smote.best_params_}\")\n",
    "y_pred_under_smote_tuned = grid_gb_under_smote.best_estimator_.predict(X_test)\n",
    "y_prob_under_smote_tuned = grid_gb_under_smote.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_under_smote_tuned, y_prob_under_smote_tuned, \"Gradient Boosting - Under+SMOTE (Otimizado)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados: com ajuste de hiperparâmetros (ADASYN + Undersampling - Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_under_adasyn_gb = {\n",
    "    'under_sampler__sampling_strategy': [0.5, 0.7, 0.9],\n",
    "    'over_sampler__n_neighbors': [3, 5, 7, 10],\n",
    "    'gradientboost__n_estimators': [50, 100, 200],\n",
    "    'gradientboost__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gradientboost__max_depth': [3, 5, 7],\n",
    "    'gradientboost__min_samples_split': [2, 5],\n",
    "    'gradientboost__min_samples_leaf': [1, 3],\n",
    "    'gradientboost__subsample': [0.8, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Otimização para ADASYN + Undersampling (Gradient Boosting) ---\n",
    "print(\"\\nOtimizando Pipeline com ADASYN + Undersampling (Gradient Boosting)...\")\n",
    "grid_gb_under_adasyn = RandomizedSearchCV(pipeline_under_adasyn_gb, param_grid_under_adasyn_gb, n_iter=10, cv=cv_strategy,\n",
    "                          scoring=scorer, n_jobs=-1, verbose=1, random_state=42)\n",
    "\n",
    "grid_gb_under_adasyn.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Melhores parâmetros para ADASYN + Undersampling (Gradient Boosting): {grid_gb_under_adasyn.best_params_}\")\n",
    "y_pred_under_adasyn_tuned = grid_gb_under_adasyn.best_estimator_.predict(X_test)\n",
    "y_prob_under_adasyn_tuned = grid_gb_under_adasyn.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "evaluate_model(y_test, y_pred_under_adasyn_tuned, y_prob_under_adasyn_tuned, \"Gradient Boosting - Under+ADASYN (Otimizado)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
