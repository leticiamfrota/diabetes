import streamlit as st
import pandas as pd
import joblib
import os
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import json

from sklearn.metrics import ConfusionMatrixDisplay

def download_infos_modelo():
    """
    Faz o download das infos do modelo mais recente.
    """
    url = "https://github.com/matheusfinger/model-pipeline-brfss/raw/main/model_metrics.json" # URL para o arquivo raw JSON
    output_file = "infos_model.json"

    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()  # Verifica se houve erro na requisiÃ§Ã£o
        
        with open(output_file, 'wb') as file:
            for chunk in response.iter_content(chunk_size=8192):
                file.write(chunk)
        
        st.success(f"Arquivo de mÃ©tricas baixado com sucesso: {output_file}")
    except requests.exceptions.RequestException as e:
        st.error(f"Erro ao baixar o arquivo de mÃ©tricas: {e}. Verifique sua conexÃ£o com a internet ou o URL.")
        st.stop()


def show_model_details_page():
    st.title("ğŸ”¬ Detalhes sobre a AvaliaÃ§Ã£o do Modelo PrÃ©-treinado")

    with st.spinner("Baixando as informaÃ§Ãµes do modelo..."):
        download_infos_modelo()

    metrics_file = "infos_model.json"

    try:
        with open(metrics_file, 'r') as f:
            model_data = json.load(f)
        st.success(f"Dados do modelo e mÃ©tricas carregados com sucesso de: `{metrics_file}`")
    except FileNotFoundError:
        st.error(f"Erro: O arquivo de mÃ©tricas '{metrics_file}' nÃ£o foi encontrado apÃ³s o download.")
        st.info("Por favor, verifique se o arquivo foi baixado corretamente.")
        return
    except json.JSONDecodeError as e:
        st.error(f"Erro ao decodificar o arquivo JSON: {e}. O arquivo pode estar corrompido ou mal formatado.")
        return
    except Exception as e:
        st.error(f"Ocorreu um erro ao carregar os dados do modelo: {e}")
        return

    # Extraindo as mÃ©tricas e informaÃ§Ãµes do modelo
    metrics = model_data.get("metrics", {})
    model_info = model_data.get("model_info", {})
    ano = model_data.get("ano", "N/A")

    st.header("1. InformaÃ§Ãµes do Modelo")
    st.write(f"Este modelo de Machine Learning foi avaliado em **{ano}**.")
    st.write(f"O tipo de modelo utilizado Ã©: **{model_info.get('model_name', 'NÃ£o especificado')}**.")
    
    st.subheader("ParÃ¢metros Principais do Modelo:")
    if model_info.get('model_params'):
        st.markdown("Estes sÃ£o os ajustes (parÃ¢metros) com os quais o modelo foi configurado para funcionar:")
        for param, value in model_info['model_params'].items():
            st.markdown(f"- **`{param}`**: `{value}`")
    else:
        st.info("ParÃ¢metros do modelo nÃ£o disponÃ­veis no arquivo JSON.")

    st.markdown("---")

    st.header("2. Resultados da AvaliaÃ§Ã£o no Conjunto de Teste")
    st.markdown("Para entender quÃ£o bem nosso modelo funciona, olhamos para algumas medidas-chave:")

    accuracy = metrics.get("accuracy_score", "N/A")
    recall = metrics.get("recall_score", "N/A")
    f1_score_val = metrics.get("f1_score", "N/A")
    roc_auc = metrics.get("roc_auc_score", "N/A")
    
    col_metric, col_explanation = st.columns([1, 2])

    with col_metric:
        st.metric(label="AcurÃ¡cia Geral", value=f"{accuracy:.2%}" if isinstance(accuracy, (int, float)) else accuracy)
    with col_explanation:
        st.markdown("A **AcurÃ¡cia** nos diz a porcentagem total de previsÃµes corretas que o modelo fez. Por exemplo, se a acurÃ¡cia Ã© de 73%, significa que o modelo acertou em 73% de todas as suas previsÃµes (tanto de quem tem diabetes quanto de quem nÃ£o tem).")

    st.markdown("---")
    
    with col_metric:
        st.metric(label="Recall (Sensibilidade para Diabetes)", value=f"{recall:.2%}" if isinstance(recall, (int, float)) else recall)
    with col_explanation:
        st.markdown("O **Recall** (ou Sensibilidade) Ã© especialmente importante para nÃ³s. Ele mede a capacidade do modelo de identificar corretamente todos os casos *positivos* de diabetes. Por exemplo, um recall de 65% significa que, de todas as pessoas que *realmente tÃªm* diabetes, o modelo conseguiu identificar 65% delas. Queremos que este valor seja alto para nÃ£o deixarmos muitos casos de diabetes passarem despercebidos.")

    st.markdown("---")

    with col_metric:
        st.metric(label="F1-Score", value=f"{f1_score_val:.4f}" if isinstance(f1_score_val, (int, float)) else f1_score_val)
    with col_explanation:
        st.markdown("O **F1-Score** Ã© um equilÃ­brio entre a 'precisÃ£o' (quantos dos casos que o modelo previu como diabetes realmente tinham diabetes) e o 'recall'. Ã‰ uma boa medida Ãºnica para ver se o modelo estÃ¡ bom em ambos os aspectos, especialmente quando as classes (diabetes vs. nÃ£o-diabetes) nÃ£o sÃ£o igualmente distribuÃ­das.")

    st.markdown("---")
    
    with col_metric:
        st.metric(label="ROC AUC", value=f"{roc_auc:.4f}" if isinstance(roc_auc, (int, float)) else roc_auc)
    with col_explanation:
        st.markdown("O **ROC AUC** (Area Under the Receiver Operating Characteristic Curve) avalia a capacidade do modelo de distinguir entre as classes. Um valor de 0.5 sugere um desempenho aleatÃ³rio, enquanto um valor de 1.0 indica um modelo perfeito. Quanto mais prÃ³ximo de 1, melhor o modelo separa quem tem diabetes de quem nÃ£o tem.")


    col1, col2 = st.columns([1, 1.5])

    with col1:
        st.subheader("Matriz de ConfusÃ£o")
        st.markdown("A **Matriz de ConfusÃ£o** nos mostra detalhadamente os tipos de acertos e erros do modelo:")
        st.markdown("- **Verdadeiro Negativo (VN)**: O modelo previu que a pessoa *nÃ£o tem* diabetes, e ela *realmente nÃ£o tem*. (Top-left)")
        st.markdown("- **Falso Positivo (FP)**: O modelo previu que a pessoa *tem* diabetes, mas ela *nÃ£o tem*. (Top-right - um 'alarme falso')")
        st.markdown("- **Falso Negativo (FN)**: O modelo previu que a pessoa *nÃ£o tem* diabetes, mas ela *realmente tem*. (Bottom-left - um 'diagnÃ³stico perdido')")
        st.markdown("- **Verdadeiro Positivo (VP)**: O modelo previu que a pessoa *tem* diabetes, e ela *realmente tem*. (Bottom-right - um acerto importante)")
    with col2:
        if "confusion_matrix" in metrics and len(metrics["confusion_matrix"]) == 2 and len(metrics["confusion_matrix"][0]) == 2:
            cm = metrics["confusion_matrix"]
            
            fig, ax = plt.subplots(figsize=(6, 4))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,
                        xticklabels=["NÃ£o DiabÃ©tico", "DiabÃ©tico"], yticklabels=["NÃ£o DiabÃ©tico", "DiabÃ©tico"])
            ax.set_xlabel('Predito')
            ax.set_ylabel('Real')
            plt.title("Matriz de ConfusÃ£o")
            st.pyplot(fig)
            plt.close(fig)
        else:
            st.info("Dados da matriz de confusÃ£o nÃ£o disponÃ­veis no arquivo JSON para exibiÃ§Ã£o.")

    st.subheader("RelatÃ³rio de ClassificaÃ§Ã£o Detalhado")
    st.markdown("Este relatÃ³rio fornece uma visÃ£o mais aprofundada das mÃ©tricas para cada categoria (0 = NÃ£o DiabÃ©tico, 1 = DiabÃ©tico):")
    st.markdown("- **Precision (PrecisÃ£o)**: Dos casos que o modelo previu como positivos para uma classe, quantos estavam realmente corretos.")
    st.markdown("- **Recall (Sensibilidade)**: Dos casos que realmente pertencem a uma classe, quantos o modelo conseguiu identificar corretamente.")
    st.markdown("- **f1-score**: Uma mÃ©dia ponderada da PrecisÃ£o e do Recall.")
    st.markdown("- **support**: O nÃºmero de ocorrÃªncias reais de cada classe nos dados de teste.")
    st.markdown("- **macro avg**: MÃ©dia simples das mÃ©tricas por classe.")
    st.markdown("- **weighted avg**: MÃ©dia das mÃ©tricas por classe, ponderada pelo 'support' (nÃºmero de ocorrÃªncias) de cada classe.")

    col_left_spacer, col_table, col_right_spacer = st.columns([0.5, 3, 0.5])
    
    with col_table:
        if "classification_report" in metrics and isinstance(metrics["classification_report"], str):
            
            report_str = metrics["classification_report"]
            try:
                lines = report_str.strip().split('\n')
                header_line = lines[0].strip().split()
                

                data_rows = []
                for line in lines[1:]: 
                    parts = line.strip().split()
                    if not parts:
                        continue
                    if parts[0].isdigit() or parts[0] == 'accuracy' or parts[0] == 'macro' or parts[0] == 'weighted':
                        if len(parts) == 2 and parts[0] == 'accuracy': 
                            data_rows.append([parts[0], '', '', parts[1], '']) 
                        elif len(parts) >= 4: 
                            data_rows.append(parts)
                        elif len(parts) == 3 and (parts[0] == 'macro' or parts[0] == 'weighted') : 
                             data_rows.append([f"{parts[0]} {parts[1]}", parts[2], parts[3], parts[4], parts[5]]) 
                        elif len(parts) == 5:
                            data_rows.append([f"{parts[0]} {parts[1]}", parts[2], parts[3], parts[4], parts[5]])
                
                col_names = ["Category", "precision", "recall", "f1-score", "support"]
                if not header_line: 
                    report_df = pd.DataFrame(data_rows)
                else: 
                    parsed_report = {}
                    current_key = None
                    for line in lines:
                        line = line.strip()
                        if not line:
                            continue
                        
                        if 'precision' in line and 'recall' in line and 'f1-score' in line and 'support' in line:
                            header_labels = line.split()
                            continue 
                        
                        parts = line.split()
                        if not parts:
                            continue
                        
                        if parts[0].isdigit() and len(parts) == 5:
                            category = parts[0]
                            parsed_report[category] = {
                                'precision': float(parts[1]),
                                'recall': float(parts[2]),
                                'f1-score': float(parts[3]),
                                'support': int(parts[4])
                            }
                        elif parts[0] == 'accuracy' and len(parts) == 2:
                            parsed_report['accuracy'] = float(parts[1])
                        elif (parts[0] == 'macro' or parts[0] == 'weighted') and len(parts) == 5:
                             key_name = f"{parts[0]} {parts[1]}"
                             parsed_report[key_name] = {
                                'precision': float(parts[2]),
                                'recall': float(parts[3]),
                                'f1-score': float(parts[4]),
                                'support': int(parts[5])
                            }
                        
                    
                    report_df = pd.DataFrame.from_dict(parsed_report, orient='index')
                    
                    
                    if 'accuracy' in report_df.index:
                        accuracy_val = report_df.loc['accuracy'].iloc[0] 
                        report_df.loc['accuracy', ['precision', 'recall', 'f1-score', 'support']] = [accuracy_val, accuracy_val, accuracy_val, accuracy_val]
                    
                    st.dataframe(report_df.fillna('')) 

            except Exception as e:
                st.error(f"Erro ao processar o relatÃ³rio de classificaÃ§Ã£o: {e}. Exibindo como texto simples.")
                st.code(metrics["classification_report"])
        else:
            st.info("Dados do relatÃ³rio de classificaÃ§Ã£o nÃ£o disponÃ­veis no arquivo JSON para exibiÃ§Ã£o.")



# ==============================================================================
# NAVEGAÃ‡ÃƒO PRINCIPAL
# ==============================================================================
st.sidebar.title("NavegaÃ§Ã£o")
page = st.sidebar.radio("Escolha uma pÃ¡gina:", ["AnÃ¡lise de Risco (PrediÃ§Ã£o)", "Detalhes do Modelo (AvaliaÃ§Ã£o)"])

if page == "AnÃ¡lise de Risco (PrediÃ§Ã£o)":
    st.write("VÃ¡ para a pÃ¡gina de 'AnÃ¡lise de Risco (PrediÃ§Ã£o)' para usar o modelo.")
else:
    show_model_details_page()